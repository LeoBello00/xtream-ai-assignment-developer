{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the models dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My goal is to create a easy and flexible solution ready for future changes, so i will simplify everything by using a dictionary of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marta has her notebook where she trains and analyzes the models, so i will use her models to create the dictionary, i will use Pickle for saving the models and importing them so that the mangement of the two is easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: linear_regression_diamonds\n",
      "Model: {'model': LinearRegression(), 'model_name': 'linear_regression_diamonds', 'preferred_preprocessing': 0}\n",
      "Model: xgboost_diamonds\n",
      "Model: {'model': XGBRegressor(alpha=0.032805312311200055, base_score=None, booster=None,\n",
      "             callbacks=None, colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=0.7, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=True, eval_metric=None, feature_types=None,\n",
      "             gamma=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, lambda=0.00013669441189434554,\n",
      "             learning_rate=0.09548820588899541, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=6, max_leaves=None,\n",
      "             min_child_weight=5, missing=nan, monotone_constraints=None,\n",
      "             multi_strategy=None, n_estimators=964, n_jobs=None, ...), 'model_name': 'xgboost_diamonds', 'preferred_preprocessing': 1}\n"
     ]
    }
   ],
   "source": [
    "# import every model using pickle into a dictionary, by importing every pickle in the models folder\n",
    "import pandas as pd\n",
    "import pickle,os\n",
    "\n",
    "model_path = \"../history/current_models\"\n",
    "models = {}\n",
    "\n",
    "# Load the models from the models folder\n",
    "\n",
    "for file in os.listdir(model_path):\n",
    "    if file.endswith(\".pkl\"):\n",
    "        model_name = file.split(\".\")[0]\n",
    "        model = pickle.load(open(f\"{model_path}/{file}\", \"rb\"))\n",
    "        models[model_name] = model\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Model: {model}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to preprocess the new data in different ways, based on the model we are training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I changed a bit the notebook made by Marta by adding the creation of dictionaries which contain the models and their preferences about data, for example either if it likes non numerical values or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def preprocess_input_for_tree_models(input_data):\n",
    "    diamonds_processed = deepcopy(input_data)\n",
    "    # remove every line with a missing value\n",
    "    diamonds_processed = diamonds_processed[(diamonds_processed.x * diamonds_processed.y * diamonds_processed.z != 0) & (diamonds_processed.price > 0)]\n",
    "    diamonds_processed['cut'] = pd.Categorical(diamonds_processed['cut'], categories=['Fair', 'Good', 'Very Good', 'Ideal', 'Premium'], ordered=True)\n",
    "    diamonds_processed['color'] = pd.Categorical(diamonds_processed['color'], categories=['D', 'E', 'F', 'G', 'H', 'I', 'J'], ordered=True)\n",
    "    diamonds_processed['clarity'] = pd.Categorical(diamonds_processed['clarity'], categories=['IF', 'VVS1', 'VVS2', 'VS1', 'VS2', 'SI1', 'SI2', 'I1'], ordered=True)\n",
    "    return diamonds_processed\n",
    "\n",
    "def preprocess_input_for_linear_models(input_data):\n",
    "    diamonds_processed = deepcopy(input_data)\n",
    "    diamonds_processed = diamonds_processed[(diamonds_processed.x * diamonds_processed.y * diamonds_processed.z != 0) & (diamonds_processed.price > 0)]\n",
    "    diamonds_processed = diamonds_processed.dropna()\n",
    "    diamonds_processed.drop(columns=['depth', 'table', 'y', 'z'])\n",
    "    diamonds_processed = pd.get_dummies(diamonds_processed, columns=['cut', 'color', 'clarity'])\n",
    "    return diamonds_processed\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the new dataset with the old one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge new csv file with old one\n",
    "old_csv = \"../data/diamonds_1.csv\"\n",
    "def merge_csv_files(new_csv):\n",
    "    old_data = pd.read_csv(old_csv)\n",
    "    new_data = pd.read_csv(new_csv)\n",
    "    merged_data = pd.concat([old_data, new_data])\n",
    "    merged_data.to_csv(old_csv, index=False)\n",
    "    # move the new csv file to a different folder\n",
    "    new_data.to_csv(f\"../data/trash/{new_csv.split('/')[-1]}\", index=False)\n",
    "    os.remove(new_csv)\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the current models with the new data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "\n",
    "def fit_test_models(new_data):\n",
    "    print(\"Fitting models\")\n",
    "    x_train_0, x_test_0, y_train_0, y_test_0 = train_test_split(new_data[0].drop(columns=['price']), new_data[0]['price'], test_size=0.2, random_state=0)\n",
    "    x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(new_data[1].drop(columns=['price']), new_data[1]['price'], test_size=0.2, random_state=0)\n",
    "\n",
    "    for model_name,model in models.items():\n",
    "        if(model[\"preferred_preprocessing\"] == 0):\n",
    "            model[\"model\"].fit(x_train_0, np.log(y_train_0))\n",
    "            utils.save_model(model[\"model\"], model_name)\n",
    "        elif(model[\"preferred_preprocessing\"] == 1):\n",
    "            model[\"model\"].fit(x_train_1, y_train_1)\n",
    "            utils.save_model(model[\"model\"], model_name,1)\n",
    "        else:\n",
    "            print(model[\"preferred_preprocessing\"])\n",
    "            raise ValueError(\"Invalid preprocessing method\")\n",
    "    test_models((y_test_0, y_test_1, x_test_0, x_test_1))\n",
    "\n",
    "def test_models(data):\n",
    "    y_val_0, y_val_1,x_val_0,x_val_1 = data\n",
    "    results = {}\n",
    "    for model_name,model in models.items():\n",
    "        if(model[\"preferred_preprocessing\"] == 0):\n",
    "            pred = np.exp(model[\"model\"].predict(x_val_0))\n",
    "            utils.save_scores(y_val_0, pred, model_name)\n",
    "        elif(model[\"preferred_preprocessing\"] == 1):\n",
    "            pred = model[\"model\"].predict(x_val_1)\n",
    "            utils.save_scores(y_val_1, pred, model_name)\n",
    "        else:\n",
    "            print(model[\"preferred_preprocessing\"])\n",
    "            raise ValueError(\"Invalid preprocessing method\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scanning folder for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: watchdog in /Users/leonardovaia/miniconda3/envs/ml_venv/lib/python3.9/site-packages (4.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install watchdog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data_preprocessed = []\n",
    "# file_path = \"../data/new_data/diamonds_2.csv\"\n",
    "\n",
    "# new_data = pd.read_csv(file_path)\n",
    "# new_data_preprocessed_tree = preprocess_input_for_tree_models(new_data)\n",
    "# new_data_preprocessed_linear = preprocess_input_for_linear_models(new_data)\n",
    "# new_data_preprocessed.append(new_data_preprocessed_linear)\n",
    "# new_data_preprocessed.append(new_data_preprocessed_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data_preprocessed[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_test_models(new_data_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New file detected: /Users/leonardovaia/Documents/Job_interview/xtream-ai-assignment-developer/data/new_data/diamonds_3.csv\n",
      "Processing file: /Users/leonardovaia/Documents/Job_interview/xtream-ai-assignment-developer/data/new_data/diamonds_3.csv\n",
      "Fitting models\n",
      "linear_regression_diamonds\n",
      "{'r2_score': 0.9400259912530078, 'mae': 394.5549991908956}\n",
      "xgboost_diamonds\n",
      "{'r2_score': 0.9623680284898042, 'mae': 350.48988344828285}\n",
      "New file detected: /Users/leonardovaia/Documents/Job_interview/xtream-ai-assignment-developer/data/new_data/diamonds_4.csv\n",
      "Processing file: /Users/leonardovaia/Documents/Job_interview/xtream-ai-assignment-developer/data/new_data/diamonds_4.csv\n",
      "Fitting models\n",
      "linear_regression_diamonds\n",
      "{'r2_score': 0.9637801650738812, 'mae': 404.80655621850013}\n",
      "xgboost_diamonds\n",
      "{'r2_score': 0.9684146102306985, 'mae': 350.7339361809073}\n",
      "New file detected: /Users/leonardovaia/Documents/Job_interview/xtream-ai-assignment-developer/data/new_data/diamonds_5.csv\n",
      "Processing file: /Users/leonardovaia/Documents/Job_interview/xtream-ai-assignment-developer/data/new_data/diamonds_5.csv\n",
      "Fitting models\n",
      "linear_regression_diamonds\n",
      "{'r2_score': 0.9346487743457267, 'mae': 434.0909771564789}\n",
      "xgboost_diamonds\n",
      "{'r2_score': 0.9698388216369919, 'mae': 325.4846761088094}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "\n",
    "class FileEventHandler(FileSystemEventHandler):\n",
    "    def on_created(self, event):\n",
    "        if event.is_directory:\n",
    "            return\n",
    "        if event.src_path.endswith('.csv'):\n",
    "            print(f\"New file detected: {event.src_path}\")\n",
    "            process_new_file(event.src_path)\n",
    "\n",
    "def process_new_file(file_path):\n",
    "    new_data_preprocessed = []\n",
    "    # Add your data processing and model training code here\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    new_data = merge_csv_files(file_path)\n",
    "    new_data_preprocessed_tree = preprocess_input_for_tree_models(new_data)\n",
    "    new_data_preprocessed_linear = preprocess_input_for_linear_models(new_data)\n",
    "    new_data_preprocessed.append(new_data_preprocessed_linear)\n",
    "    new_data_preprocessed.append(new_data_preprocessed_tree)\n",
    "    \n",
    "    #print('new data 0', new_data_preprocessed[0].head())\n",
    "    fit_test_models(new_data_preprocessed)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"../data/new_data\"\n",
    "    event_handler = FileEventHandler()\n",
    "    observer = Observer()\n",
    "    observer.schedule(event_handler, path, recursive=False)\n",
    "    observer.start()\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        observer.stop()\n",
    "    observer.join()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
