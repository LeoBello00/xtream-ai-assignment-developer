{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data\n",
    "We already have a pre-elaboration od the data thanks to Marta, i will work with this new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "diamonds = pd.read_csv(\"../data/diamonds_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the models dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My goal is to create a easy and flexible solution ready for future changes, so i will simplify everything by using a dictionary of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marta has her notebook where she trains and analyzes the models, so i will use her models to create the dictionary, i will use Pickle for saving the models and importing them so that the mangement of the two is easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: linear_regression_diamonds\n",
      "Model: {'model': LinearRegression(), 'model_name': 'linear_regression_diamonds', 'preferred_preprocessing': 0}\n",
      "Model: xgboost_diamonds\n",
      "Model: {'model': XGBRegressor(alpha=0.15199357133143715, base_score=None, booster=None,\n",
      "             callbacks=None, colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=0.7, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=True, eval_metric=None, feature_types=None,\n",
      "             gamma=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, lambda=0.0011030416763866523,\n",
      "             learning_rate=0.03238176135427902, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=5, max_leaves=None,\n",
      "             min_child_weight=7, missing=nan, monotone_constraints=None,\n",
      "             multi_strategy=None, n_estimators=274, n_jobs=None, ...), 'model_name': 'xgboost_diamonds', 'preferred_preprocessing': 1}\n"
     ]
    }
   ],
   "source": [
    "# import every model using pickle into a dictionary, by importing every pickle in the models folder\n",
    "\n",
    "import pickle,os\n",
    "\n",
    "model_path = \"../history/current_models\"\n",
    "models = {}\n",
    "\n",
    "# Load the models from the models folder\n",
    "\n",
    "for file in os.listdir(model_path):\n",
    "    if file.endswith(\".pkl\"):\n",
    "        model_name = file.split(\".\")[0]\n",
    "        model = pickle.load(open(f\"{model_path}/{file}\", \"rb\"))\n",
    "        models[model_name] = model\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Model: {model}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to preprocess the new data in different ways, based on the model we are training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I changed a bit the notebook made by Marta by adding the creation of dictionaries which contain the models and their preferences about data, for example either if it likes non numerical values or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def preprocess_input_for_tree_models(input_data):\n",
    "    diamonds_processed = deepcopy(input_data)\n",
    "    # remove every line with a missing value\n",
    "    diamonds_processed = diamonds_processed[(diamonds_processed.x * diamonds_processed.y * diamonds_processed.z != 0) & (diamonds_processed.price > 0)]\n",
    "    diamonds_processed['cut'] = pd.Categorical(diamonds_processed['cut'], categories=['Fair', 'Good', 'Very Good', 'Ideal', 'Premium'], ordered=True)\n",
    "    diamonds_processed['color'] = pd.Categorical(diamonds_processed['color'], categories=['D', 'E', 'F', 'G', 'H', 'I', 'J'], ordered=True)\n",
    "    diamonds_processed['clarity'] = pd.Categorical(diamonds_processed['clarity'], categories=['IF', 'VVS1', 'VVS2', 'VS1', 'VS2', 'SI1', 'SI2', 'I1'], ordered=True)\n",
    "    return diamonds_processed\n",
    "\n",
    "def preprocess_input_for_linear_models(input_data):\n",
    "    diamonds_processed = deepcopy(input_data)\n",
    "    diamonds_processed = diamonds_processed[(diamonds_processed.x * diamonds_processed.y * diamonds_processed.z != 0) & (diamonds_processed.price > 0)]\n",
    "    diamonds_processed = diamonds_processed.dropna()\n",
    "    diamonds_processed.drop(columns=['depth', 'table', 'y', 'z'])\n",
    "    diamonds_processed = pd.get_dummies(diamonds_processed, columns=['cut', 'color', 'clarity'])\n",
    "    return diamonds_processed\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the current models with the new data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "validation_df = pd.read_csv(\"../data/diamonds_1.csv\")\n",
    "validation_0 = preprocess_input_for_linear_models(validation_df)\n",
    "validation_1 = preprocess_input_for_tree_models(validation_df)\n",
    "\n",
    "x_val_0 = validation_0.drop(columns=['price'])\n",
    "y_val_0 = validation_0['price']\n",
    "x_val_1 = validation_1.drop(columns=['price'])\n",
    "y_val_1 = validation_1['price']\n",
    "\n",
    "def fit_test_models(new_data):\n",
    "    print(\"Fitting models\")\n",
    "    x_train_0 = new_data[0].drop(columns=['price'])\n",
    "    y_train_0 = new_data[0]['price']\n",
    "    x_train_1 = new_data[1].drop(columns=['price'])\n",
    "    y_train_1 = new_data[1]['price']\n",
    "    for model_name,model in models.items():\n",
    "        if(model[\"preferred_preprocessing\"] == 0):\n",
    "            model[\"model\"].fit(x_train_0, np.log(y_train_0))\n",
    "            utils.save_model(model, model_name)\n",
    "        elif(model[\"preferred_preprocessing\"] == 1):\n",
    "            model[\"model\"].fit(x_train_1, y_train_1)\n",
    "            utils.save_model(model, model_name,1)\n",
    "        else:\n",
    "            print(model[\"preferred_preprocessing\"])\n",
    "            raise ValueError(\"Invalid preprocessing method\")\n",
    "    test_models()\n",
    "\n",
    "def test_models():\n",
    "    results = {}\n",
    "    for model_name,model in models.items():\n",
    "        if(model[\"preferred_preprocessing\"] == 0):\n",
    "            pred = np.exp(model[\"model\"].predict(x_val_0))\n",
    "            utils.save_scores(y_val_0, pred, model_name)\n",
    "        elif(model[\"preferred_preprocessing\"] == 1):\n",
    "            pred = model[\"model\"].predict(x_val_1)\n",
    "            utils.save_scores(y_val_1, pred, model_name)\n",
    "        else:\n",
    "            print(model[\"preferred_preprocessing\"])\n",
    "            raise ValueError(\"Invalid preprocessing method\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scanning folder for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: watchdog in /Users/leonardovaia/miniconda3/envs/ml_venv/lib/python3.9/site-packages (4.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install watchdog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data_preprocessed = []\n",
    "# file_path = \"../data/new_data/diamonds_2.csv\"\n",
    "\n",
    "# new_data = pd.read_csv(file_path)\n",
    "# new_data_preprocessed_tree = preprocess_input_for_tree_models(new_data)\n",
    "# new_data_preprocessed_linear = preprocess_input_for_linear_models(new_data)\n",
    "# new_data_preprocessed.append(new_data_preprocessed_linear)\n",
    "# new_data_preprocessed.append(new_data_preprocessed_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data_preprocessed[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_test_models(new_data_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New file detected: /Users/leonardovaia/Documents/Job_interview/xtream-ai-assignment-developer/data/new_data/diamonds_2.csv\n",
      "Processing file: /Users/leonardovaia/Documents/Job_interview/xtream-ai-assignment-developer/data/new_data/diamonds_2.csv\n",
      "Fitting models\n",
      "linear_regression_diamonds\n",
      "{'r2_score': 0.9480622269624593, 'mae': 409.8306245856353}\n",
      "xgboost_diamonds\n",
      "{'r2_score': 0.9575192706274285, 'mae': 382.9641550730418}\n",
      "New file detected: /Users/leonardovaia/Documents/Job_interview/xtream-ai-assignment-developer/data/new_data/diamonds_3.csv\n",
      "Processing file: /Users/leonardovaia/Documents/Job_interview/xtream-ai-assignment-developer/data/new_data/diamonds_3.csv\n",
      "Fitting models\n",
      "linear_regression_diamonds\n",
      "{'r2_score': 0.9383460275734579, 'mae': 425.2228152355467}\n",
      "xgboost_diamonds\n",
      "{'r2_score': 0.9532793502986119, 'mae': 406.11942951862994}\n",
      "New file detected: /Users/leonardovaia/Documents/Job_interview/xtream-ai-assignment-developer/data/new_data/diamonds_4.csv\n",
      "Processing file: /Users/leonardovaia/Documents/Job_interview/xtream-ai-assignment-developer/data/new_data/diamonds_4.csv\n",
      "Fitting models\n",
      "linear_regression_diamonds\n",
      "{'r2_score': 0.9568403738630558, 'mae': 406.6059470495262}\n",
      "xgboost_diamonds\n",
      "{'r2_score': 0.9584418647464977, 'mae': 379.9550252767416}\n",
      "New file detected: /Users/leonardovaia/Documents/Job_interview/xtream-ai-assignment-developer/data/new_data/diamonds_5.csv\n",
      "Processing file: /Users/leonardovaia/Documents/Job_interview/xtream-ai-assignment-developer/data/new_data/diamonds_5.csv\n",
      "Fitting models\n",
      "linear_regression_diamonds\n",
      "{'r2_score': 0.950770374106836, 'mae': 414.38077644848084}\n",
      "xgboost_diamonds\n",
      "{'r2_score': 0.9499916837535685, 'mae': 400.48883441547014}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "\n",
    "class FileEventHandler(FileSystemEventHandler):\n",
    "    def on_created(self, event):\n",
    "        if event.is_directory:\n",
    "            return\n",
    "        if event.src_path.endswith('.csv'):\n",
    "            print(f\"New file detected: {event.src_path}\")\n",
    "            process_new_file(event.src_path)\n",
    "\n",
    "def process_new_file(file_path):\n",
    "    new_data_preprocessed = []\n",
    "    # Add your data processing and model training code here\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    new_data = pd.read_csv(file_path)\n",
    "    new_data_preprocessed_tree = preprocess_input_for_tree_models(new_data)\n",
    "    new_data_preprocessed_linear = preprocess_input_for_linear_models(new_data)\n",
    "    new_data_preprocessed.append(new_data_preprocessed_linear)\n",
    "    new_data_preprocessed.append(new_data_preprocessed_tree)\n",
    "    \n",
    "    #print('new data 0', new_data_preprocessed[0].head())\n",
    "    fit_test_models(new_data_preprocessed)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"../data/new_data\"\n",
    "    event_handler = FileEventHandler()\n",
    "    observer = Observer()\n",
    "    observer.schedule(event_handler, path, recursive=False)\n",
    "    observer.start()\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        observer.stop()\n",
    "    observer.join()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../data/diamonds.csv\n",
      "Fitting models\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocess_new_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/diamonds.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m, in \u001b[0;36mprocess_new_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m new_data_preprocessed\u001b[38;5;241m.\u001b[39mappend(new_data_preprocessed_tree)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#print('new data 0', new_data_preprocessed[0].head())\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mfit_test_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_data_preprocessed\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mfit_test_models\u001b[0;34m(new_data)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name,model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(model[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreferred_preprocessing\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m(x_train_0, np\u001b[38;5;241m.\u001b[39mlog(y_train_0))\n\u001b[1;32m     12\u001b[0m         utils\u001b[38;5;241m.\u001b[39msave_model(model, model_name)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m(model[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreferred_preprocessing\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "process_new_file(\"../data/diamonds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
